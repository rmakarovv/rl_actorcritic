{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "    Author: Roman Makarov\n",
        "    e-mail: mcronomus@gmail.com"
      ],
      "metadata": {
        "id": "CCOCBRAQIFwn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr5ohicqUu8J"
      },
      "source": [
        "# Task Description\n",
        "\n",
        "You have to move several cargos across the gridworld into a common desirable rectangle area. There are several cargos, you can move each of them either horizontally or vertically by one cell up or down. Size of the overall world may vary, as well as placement of the cargos and desirable area. The game ends when all the cargos are in the desirable area and do not overlap.\n",
        "\n",
        "Reward function:\n",
        "* for each cell of cargo placed in desirable area in the end of the turn, reward is $+1$\n",
        "* for each cell where cargos overlap in the end of the turn, reward is $-1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmbAGt_ldNvo"
      },
      "source": [
        "`{infile}.txt` file with the field description. Elements of the field are separated by space. For example:\n",
        "\n",
        "```\n",
        "0 0 2 2 0\n",
        "0 r r r 0\n",
        "0 r r r 1\n",
        "0 r r r 1\n",
        "0 0 0 0 1\n",
        "```\n",
        "\n",
        "* `0` - blank space, we may move objects here\n",
        "* `r` - desirable area, we should move objects here\n",
        "* `1`, `2`, ... - actual object shapes, does not change, moved as a solid object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yMWo5Q2c8W6"
      },
      "source": [
        "In your output file, you have to record lines in the following manner: `{id} {D/U/R/L}`. For the given example, possible sequence of steps is:\n",
        "```\n",
        "2 D\n",
        "1 L\n",
        "1 U\n",
        "2 L\n",
        "```\n",
        "\n",
        "Here the rewards are:\n",
        "1. +2 for 2 cells of `2`\n",
        "2. +4 for 2 cells of `2` and 2 cells of `1`\n",
        "3. +3 (= +4 - 1) for 2 cells of `2` and 2 cells of `1` and -1 overlapping cell\n",
        "4. **The end**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Solution"
      ],
      "metadata": {
        "id": "GyEvjsE84f1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data preparation"
      ],
      "metadata": {
        "id": "n23LxH3p8mTA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NFVTjr5vqvQ"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "from typing import Dict, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2L1VsoXTvpTX"
      },
      "outputs": [],
      "source": [
        "def read_world(file_):\n",
        "    world_ = []\n",
        "    for line in open(file_).readlines():\n",
        "        world_.append([_x for _x in line.split()])\n",
        "\n",
        "    # Find the boundaries of the desirable area\n",
        "    top_left = (-1, -1)\n",
        "    bottom_right = (-1, -1)\n",
        "    # And coordinates of the cargos\n",
        "    cargos_coordinates = dict()\n",
        "\n",
        "    for i_ in range(len(world_)):\n",
        "        for j_ in range(len(world_[i_])):\n",
        "            if world_[i_][j_] == 'r':\n",
        "                if top_left[0] == -1:\n",
        "                    top_left = [i_, j_]\n",
        "                bottom_right = [i_, j_]\n",
        "            elif world_[i_][j_] != '0':\n",
        "                index_ = int(world_[i_][j_])\n",
        "                if index_ not in cargos_coordinates:\n",
        "                    cargos_coordinates[index_] = list()\n",
        "                cargos_coordinates[index_].append([i_, j_])\n",
        "\n",
        "    return world_, cargos_coordinates, top_left, bottom_right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvRqxa8wxgpy"
      },
      "outputs": [],
      "source": [
        "def prepare_data(path_to_infile):\n",
        "    # reading and creating the world\n",
        "    world_shape, cargos_coordinates, top_left, bottom_right = read_world(path_to_infile)\n",
        "\n",
        "    cargos_corners = dict()\n",
        "    for cargo in cargos_coordinates:\n",
        "        top, bottom, left, right = 0, 0, 0, 0\n",
        "        for i in range(len(cargos_coordinates[cargo])):\n",
        "            if cargos_coordinates[cargo][top][0] > cargos_coordinates[cargo][i][0]:\n",
        "                top = i\n",
        "            if cargos_coordinates[cargo][bottom][0] < cargos_coordinates[cargo][i][0]:\n",
        "                bottom = i\n",
        "            if cargos_coordinates[cargo][left][1] > cargos_coordinates[cargo][i][1]:\n",
        "                left = i\n",
        "            if cargos_coordinates[cargo][right][1] > cargos_coordinates[cargo][i][1]:\n",
        "                right = i\n",
        "\n",
        "        cargos_corners[cargo] = [cargos_coordinates[cargo][top][0], cargos_coordinates[cargo][right][1],\n",
        "                                 cargos_coordinates[cargo][bottom][0], cargos_coordinates[cargo][left][1]]\n",
        "\n",
        "    return world_shape, cargos_coordinates, cargos_corners, top_left, bottom_right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joNNUzqu-8ky"
      },
      "outputs": [],
      "source": [
        "def print_world(world_shape, cargos_coordinates, top_left, bottom_right):\n",
        "    printed_world = [[0 for y in range(len(x))] for x in world_shape]\n",
        "\n",
        "    for i in range(top_left[0], bottom_right[0] + 1):\n",
        "        for j in range(top_left[1], bottom_right[1] + 1):\n",
        "            printed_world[i][j] = 'r'\n",
        "\n",
        "    for Cargo in cargos_coordinates:\n",
        "        for coordinates in cargos_coordinates[Cargo]:\n",
        "            printed_world[coordinates[0]][coordinates[1]] = Cargo\n",
        "\n",
        "    for line in printed_world:\n",
        "        print(*line)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###A function for generating random samples for training"
      ],
      "metadata": {
        "id": "1hlP5O-rHShN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QkSdkHBNJvP"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def generate_random_map(file_name, min_n=4, max_n=6):\n",
        "    done = False\n",
        "    while not done:\n",
        "        done = True\n",
        "        n = random.randint(min_n, max_n)\n",
        "        a = [['0' for i in range(n)] for j in range(n)]\n",
        "\n",
        "        first_x = random.randint(0, n // 2)\n",
        "        first_y = random.randint(0, n // 2)\n",
        "\n",
        "        second_x = random.randint(first_x + 1, min(n, first_x + 1 + n // 2))\n",
        "        second_y = random.randint(first_y + 1, min(n, first_y + 1 + n // 2))\n",
        "\n",
        "        for X in range(first_x, second_x):\n",
        "            for Y in range(first_y, second_y):\n",
        "                a[X][Y] = 'r'\n",
        "\n",
        "        size_x = second_x - first_x\n",
        "        size_y = second_y - first_y\n",
        "\n",
        "        generated = True\n",
        "        current_id = 0\n",
        "        count_cargos = 0\n",
        "        while generated:\n",
        "            if size_x <= 0 or size_y <= 0:\n",
        "                break\n",
        "\n",
        "            current_id += 1\n",
        "            generated = False\n",
        "\n",
        "            n_trials = 1000\n",
        "            for _ in range(n_trials):\n",
        "                success = True\n",
        "                type_ = random.randint(0, 1)\n",
        "                x_len, y_len = 1, 1\n",
        "                if type_ == 0:\n",
        "                    x_len = random.randint(1, size_x)\n",
        "                else:\n",
        "                    y_len = random.randint(1, size_y)\n",
        "\n",
        "                if _ >= n_trials - 50:\n",
        "                    x_len, y_len = 1, 1\n",
        "\n",
        "                trial_x = random.randint(0, n - 1)\n",
        "                trial_y = random.randint(0, n - 1)\n",
        "\n",
        "                if trial_x + x_len >= n or trial_y + y_len >= n:\n",
        "                    continue\n",
        "\n",
        "                for X in range(trial_x, trial_x + x_len):\n",
        "                    for Y in range(trial_y, trial_y + y_len):\n",
        "                        if a[X][Y] != '0':\n",
        "                            success = False\n",
        "                    if not success:\n",
        "                        break\n",
        "\n",
        "                if success:\n",
        "                    for X in range(trial_x, trial_x + x_len):\n",
        "                        for Y in range(trial_y, trial_y + y_len):\n",
        "                            a[X][Y] = str(current_id)\n",
        "\n",
        "                    count_cargos += 1\n",
        "                    generated = True\n",
        "                    size_x -= y_len\n",
        "                    size_y -= x_len\n",
        "\n",
        "                    break\n",
        "        if not count_cargos:\n",
        "            done = False\n",
        "            continue\n",
        "\n",
        "        with open(file_name, 'w') as f:\n",
        "            for line in a:\n",
        "                for x in line:\n",
        "                    f.write(f'{x} ')\n",
        "                f.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Neural Network"
      ],
      "metadata": {
        "id": "0kMEG9m78hT3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXnDF0pLvEJn"
      },
      "outputs": [],
      "source": [
        "eps = 0.000001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7M1bhPzTXSs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        # 8 input coordinates\n",
        "        self.affine = nn.Linear(8, 128)\n",
        "\n",
        "        self.middle_layer = nn.Linear(128, 128)\n",
        "\n",
        "        self.action_layer = nn.Linear(128, 4)\n",
        "        self.value_layer = nn.Linear(128, 1)\n",
        "\n",
        "        self.logprobs = []\n",
        "        self.state_values = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, state):\n",
        "        state = torch.from_numpy(state).float()\n",
        "\n",
        "        state = self.affine(state)\n",
        "        # state = torch.sigmoid(state)\n",
        "        state = F.relu(state, inplace=False)\n",
        "        state = self.middle_layer(state)\n",
        "        # state = torch.sigmoid(state)\n",
        "        state = F.relu(state, inplace=False)\n",
        "\n",
        "        state_value = self.value_layer(state)\n",
        "\n",
        "        act_l = self.action_layer(state)\n",
        "        action_probs = F.softmax(act_l.add(eps), dim=0)\n",
        "        action_distribution = Categorical(action_probs)\n",
        "        action = action_distribution.sample()\n",
        "\n",
        "        self.logprobs.append(action_distribution.log_prob(action))\n",
        "        self.state_values.append(state_value)\n",
        "\n",
        "        return action.item()\n",
        "\n",
        "    def calculateLoss(self, gamma=0.99):\n",
        "        # calculating discounted rewards:\n",
        "        rewards = []\n",
        "        dis_reward = 0\n",
        "        for reward in self.rewards[::-1]:\n",
        "            dis_reward = reward + gamma * dis_reward\n",
        "            rewards.insert(0, dis_reward)\n",
        "        rewards_length = len(rewards)\n",
        "\n",
        "        # normalizing the rewards:\n",
        "        rewards = torch.tensor(rewards)\n",
        "        # Somehow normalizing rewards gives none in their value :(\n",
        "        # rewards = (rewards - rewards.mean()) / (rewards.std() + eps)\n",
        "        rewards.resize_(rewards_length, 1)\n",
        "\n",
        "        loss = 0\n",
        "        for logprob, value, reward in zip(self.logprobs, self.state_values, rewards):\n",
        "            advantage = reward - value.item()\n",
        "            action_loss = -logprob * advantage\n",
        "            value_loss = F.smooth_l1_loss(value, reward)\n",
        "            loss += (action_loss + value_loss)\n",
        "        return loss\n",
        "\n",
        "    def clearMemory(self):\n",
        "        del self.logprobs[:]\n",
        "        del self.state_values[:]\n",
        "        del self.rewards[:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train and test functions"
      ],
      "metadata": {
        "id": "u9f6Lhjt8q_A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kleOIB9wN9c"
      },
      "outputs": [],
      "source": [
        "def test(policy, gamma, input_file, DEBUG=False, DEBUG2=False):\n",
        "    policy.eval()\n",
        "\n",
        "    world_shape, cargos_coordinates, cargos_corners, top_left, bottom_right = prepare_data(input_file)\n",
        "\n",
        "    moves = []\n",
        "\n",
        "    state = world_shape\n",
        "\n",
        "    running_reward = 0\n",
        "    total_cargos_length = 0\n",
        "    total_number_of_actions = 0\n",
        "    total_cargos_done = 0\n",
        "\n",
        "    Total_reward = 0\n",
        "\n",
        "    Total_length = 0\n",
        "    for cargo_i in cargos_corners:\n",
        "        Total_length += len(cargos_coordinates[cargo_i])\n",
        "\n",
        "    counted_cargos = set()\n",
        "    done_cargos = set()\n",
        "\n",
        "    while running_reward != Total_length:\n",
        "        for cargo_i in cargos_corners:\n",
        "\n",
        "            if cargo_i not in counted_cargos:\n",
        "                counted_cargos.add(cargo_i)\n",
        "                total_cargos_length += len(cargos_coordinates[cargo_i])\n",
        "\n",
        "            for t in range(1000):\n",
        "                Current_state = []\n",
        "                for coordinate_ in top_left:\n",
        "                    Current_state.append(coordinate_)\n",
        "                for coordinate_ in bottom_right:\n",
        "                    Current_state.append(coordinate_)\n",
        "                for coordinate_ in cargos_corners[cargo_i]:\n",
        "                    Current_state.append(coordinate_)\n",
        "                Current_state = np.array(Current_state).flatten()\n",
        "\n",
        "                if DEBUG2:\n",
        "                    print_world(world_shape, cargos_coordinates, cargos_corners, top_left, bottom_right)\n",
        "\n",
        "                action = 0\n",
        "                valid_action = False\n",
        "                count_tries = 0\n",
        "                while not valid_action and count_tries < 100:\n",
        "                    count_tries += 1\n",
        "                    valid_action = True\n",
        "\n",
        "                    action = policy(Current_state)\n",
        "                    change_x = (action % 2) * (1 if action in (1, 2) else -1)\n",
        "                    change_y = (1 - action % 2) * (1 if action in (1, 2) else -1)\n",
        "                    for coordinate in cargos_coordinates[cargo_i]:\n",
        "                        valid_action &= (coordinate[0] + change_x >= 0 and coordinate[0] + change_x < len(world_shape))\n",
        "                        valid_action &= (coordinate[1] + change_y >= 0 and coordinate[1] + change_y < len(world_shape[0]))\n",
        "\n",
        "                if not valid_action:\n",
        "                    break\n",
        "\n",
        "                write_action = 'U'\n",
        "                if change_x == 1:\n",
        "                    write_action = 'D'\n",
        "                elif change_y == 1:\n",
        "                    write_action = 'R'\n",
        "                elif change_y == -1:\n",
        "                    write_action = 'L'\n",
        "\n",
        "                moves.append((cargo_i, write_action))\n",
        "                total_number_of_actions += 1\n",
        "\n",
        "                # Changing the state after the move\n",
        "                for coordinate in cargos_coordinates[cargo_i]:\n",
        "                    coordinate[0] += change_x\n",
        "                    coordinate[1] += change_y\n",
        "\n",
        "                # Calculating the reward\n",
        "                current_map = [[0 for y in range(len(x))] for x in world_shape]\n",
        "                for Cargo in cargos_coordinates:\n",
        "                    for coordinates in cargos_coordinates[Cargo]:\n",
        "                        current_map[coordinates[0]][coordinates[1]] += 1\n",
        "\n",
        "                reward = 0\n",
        "                for I in range(len(current_map)):\n",
        "                    for J in range(len(current_map[I])):\n",
        "                        if current_map[I][J] > 1:\n",
        "                            reward -= 1\n",
        "                        if I >= top_left[0] and J >= top_left[1] and I <= bottom_right[0] and J <= bottom_right[1]:\n",
        "                            reward += (1 if current_map[I][J] > 0 else 0)\n",
        "\n",
        "                Total_reward += reward\n",
        "                # Determining if we are done or not\n",
        "                done = (reward == (len(cargos_coordinates[cargo_i]) + total_cargos_done))\n",
        "\n",
        "                policy.rewards.append(reward)\n",
        "                running_reward = reward\n",
        "\n",
        "                if done:\n",
        "                    if cargo_i not in done_cargos:\n",
        "                        done_cargos.add(cargo_i)\n",
        "                        total_cargos_done += len(cargos_coordinates[cargo_i])\n",
        "\n",
        "                    policy.clearMemory()\n",
        "                    break\n",
        "\n",
        "    if DEBUG:\n",
        "        print(f'Path length: {total_number_of_actions}')\n",
        "    if DEBUG2:\n",
        "        print_world(world_shape, cargos_coordinates, cargos_corners, top_left, bottom_right)\n",
        "\n",
        "    return moves, Total_reward"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REWARD_VALUE = 100\n",
        "Max_t = 1000\n",
        "\n",
        "def train(policy, optimizer, gamma, input_file, DEBUG=False, DEBUG2=False):\n",
        "    world_shape, cargos_coordinates, cargos_corners, top_left, bottom_right = prepare_data(input_file)\n",
        "\n",
        "    state = world_shape\n",
        "\n",
        "    running_reward = 0\n",
        "    total_cargos_length = 0\n",
        "    total_number_of_actions = 0\n",
        "    total_cargos_done = 0\n",
        "\n",
        "    Total_length = 0\n",
        "    for cargo_i in cargos_corners:\n",
        "        Total_length += len(cargos_coordinates[cargo_i])\n",
        "\n",
        "    counted_cargos = set()\n",
        "    done_cargos = set()\n",
        "    while running_reward < Total_length * REWARD_VALUE:\n",
        "        for cargo_i in cargos_corners:\n",
        "            if cargo_i not in counted_cargos:\n",
        "                counted_cargos.add(cargo_i)\n",
        "                total_cargos_length += len(cargos_coordinates[cargo_i])\n",
        "\n",
        "            for t in range(Max_t):\n",
        "                Current_state = []\n",
        "                for coordinate_ in top_left:\n",
        "                    Current_state.append(coordinate_)\n",
        "                for coordinate_ in bottom_right:\n",
        "                    Current_state.append(coordinate_)\n",
        "                for coordinate_ in cargos_corners[cargo_i]:\n",
        "                    Current_state.append(coordinate_)\n",
        "                Current_state = np.array(Current_state).flatten()\n",
        "\n",
        "                if DEBUG2:\n",
        "                    print_world(world_shape, cargos_coordinates, top_left, bottom_right)\n",
        "\n",
        "                action = 0\n",
        "                valid_action = False\n",
        "                count_tries = 0\n",
        "                while not valid_action and count_tries < 100:\n",
        "                    count_tries += 1\n",
        "                    valid_action = True\n",
        "\n",
        "                    action = policy(Current_state)\n",
        "                    change_x = (action % 2) * (1 if action in (1, 2) else -1)\n",
        "                    change_y = (1 - action % 2) * (1 if action in (1, 2) else -1)\n",
        "                    for coordinate in cargos_coordinates[cargo_i]:\n",
        "                        valid_action &= (coordinate[0] + change_x >= 0 and coordinate[0] + change_x < len(world_shape))\n",
        "                        valid_action &= (coordinate[1] + change_y >= 0 and coordinate[1] + change_y < len(world_shape[0]))\n",
        "\n",
        "                if not valid_action:\n",
        "                    break\n",
        "\n",
        "                total_number_of_actions += 1\n",
        "\n",
        "                prev_distance = 10 ** 9\n",
        "                for coordinate in cargos_coordinates[cargo_i]:\n",
        "                    prev_distance = min(prev_distance, abs(coordinate[0] - top_left[0]) + abs(coordinate[1] - top_left[1]))\n",
        "                    prev_distance = min(prev_distance, abs(coordinate[0] - bottom_right[0]) + abs(coordinate[1] - bottom_right[1]))\n",
        "                    prev_distance = min(prev_distance, abs(coordinate[0] - top_left[0]) + abs(coordinate[1] - bottom_right[1]))\n",
        "                    prev_distance = min(prev_distance, abs(coordinate[0] - bottom_right[0]) + abs(coordinate[1] - top_left[1]))\n",
        "                if coordinate[0] >= top_left[0] and coordinate[1] >= top_left[1] and coordinate[0] <= bottom_right[0] and coordinate[1] <= bottom_right[1]:\n",
        "                    prev_distance = 0\n",
        "\n",
        "                # Changing the state after the move\n",
        "                for coordinate in cargos_coordinates[cargo_i]:\n",
        "                    coordinate[0] += change_x\n",
        "                    coordinate[1] += change_y\n",
        "\n",
        "                cur_distance = 10 ** 9\n",
        "                for coordinate in cargos_coordinates[cargo_i]:\n",
        "                    cur_distance = min(cur_distance, abs(coordinate[0] - top_left[0]) + abs(coordinate[1] - top_left[1]))\n",
        "                    cur_distance = min(cur_distance, abs(coordinate[0] - bottom_right[0]) + abs(coordinate[1] - bottom_right[1]))\n",
        "                    cur_distance = min(cur_distance, abs(coordinate[0] - top_left[0]) + abs(coordinate[1] - bottom_right[1]))\n",
        "                    cur_distance = min(cur_distance, abs(coordinate[0] - bottom_right[0]) + abs(coordinate[1] - top_left[1]))\n",
        "                if coordinate[0] >= top_left[0] and coordinate[1] >= top_left[1] and coordinate[0] <= bottom_right[0] and coordinate[1] <= bottom_right[1]:\n",
        "                    cur_distance = 0\n",
        "\n",
        "                # Calculating the reward\n",
        "                current_map = [[0 for y in range(len(x))] for x in world_shape]\n",
        "                for Cargo in cargos_coordinates:\n",
        "                    for coordinates in cargos_coordinates[Cargo]:\n",
        "                        current_map[coordinates[0]][coordinates[1]] += 1\n",
        "\n",
        "                reward = 0\n",
        "                if cur_distance != 0:\n",
        "                    reward += REWARD_VALUE / (cur_distance + 1)\n",
        "\n",
        "                for I in range(len(current_map)):\n",
        "                    for J in range(len(current_map[I])):\n",
        "                        if current_map[I][J] > 1:\n",
        "                            reward -= 1\n",
        "                        if I >= top_left[0] and J >= top_left[1] and I <= bottom_right[0] and J <= bottom_right[1]:\n",
        "                            reward += (1 * REWARD_VALUE if current_map[I][J] > 0 else 0)\n",
        "\n",
        "                if cur_distance > prev_distance and cur_distance != 0:\n",
        "                    reward -= 2 * REWARD_VALUE / (cur_distance + 1)\n",
        "\n",
        "                # Determining if we are done or not\n",
        "                done = (reward >= (len(cargos_coordinates[cargo_i]) + total_cargos_done) * REWARD_VALUE)\n",
        "\n",
        "                policy.rewards.append(reward)\n",
        "                running_reward = reward\n",
        "\n",
        "                if done:\n",
        "                    if cargo_i not in done_cargos:\n",
        "                        done_cargos.add(cargo_i)\n",
        "                        total_cargos_done += len(cargos_coordinates[cargo_i])\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss = policy.calculateLoss(gamma)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    policy.clearMemory()\n",
        "                    break\n",
        "                elif t + 1 == Max_t:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss = policy.calculateLoss(gamma)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    policy.clearMemory()\n",
        "\n",
        "    if DEBUG:\n",
        "        print(f'Path length: {total_number_of_actions} reward: {running_reward}')\n",
        "    if DEBUG2:\n",
        "        print_world(world_shape, cargos_coordinates, top_left, bottom_right)\n",
        "        print('---------------------------------')"
      ],
      "metadata": {
        "id": "tKZfWo5dx6El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing the model"
      ],
      "metadata": {
        "id": "FuVe3Gzb8u4p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQzGQafbx9qr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "gamma = 0.99\n",
        "lr = 0.0001\n",
        "betas = (0.9, 0.999)\n",
        "\n",
        "policy = ActorCritic()\n",
        "optimizer = optim.Adam(policy.parameters(), lr=lr, betas=betas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkFFP8bwx9qr"
      },
      "outputs": [],
      "source": [
        "!rm -rf input\n",
        "!mkdir input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "684e4163-e2d0-4c68-fdae-f97c30cbf8e6",
        "id": "g9jE2RNzx9qs"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------\n",
            "Training iteration #1\n",
            "Path length: 24 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #2\n",
            "Path length: 6 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #3\n",
            "Path length: 28 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #4\n",
            "Path length: 1 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #5\n",
            "Path length: 2 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #6\n",
            "Path length: 10 reward: 400\n",
            "-----------------------------------\n",
            "Training iteration #7\n",
            "Path length: 4 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #8\n",
            "Path length: 5 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #9\n",
            "Path length: 25 reward: 300\n",
            "-----------------------------------\n",
            "Training iteration #10\n",
            "Path length: 11 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #11\n",
            "Path length: 8 reward: 300\n",
            "-----------------------------------\n",
            "Training iteration #12\n",
            "Path length: 8 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #13\n",
            "Path length: 71 reward: 300\n",
            "-----------------------------------\n",
            "Training iteration #14\n",
            "Path length: 2 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #15\n",
            "Path length: 18 reward: 300\n",
            "-----------------------------------\n",
            "Training iteration #16\n",
            "Path length: 53 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #17\n",
            "Path length: 11 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #18\n",
            "Path length: 1 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #19\n",
            "Path length: 36 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #20\n",
            "Path length: 16 reward: 300\n",
            "-----------------------------------\n",
            "Training iteration #21\n",
            "Path length: 8 reward: 300\n",
            "-----------------------------------\n",
            "Training iteration #22\n",
            "Path length: 5 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #23\n",
            "Path length: 4 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #24\n",
            "Path length: 6 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #25\n",
            "Path length: 7 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #26\n",
            "Path length: 1 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #27\n",
            "Path length: 142 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #28\n",
            "Path length: 4 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #29\n",
            "Path length: 3 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #30\n",
            "Path length: 6 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #31\n",
            "Path length: 42 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #32\n",
            "Path length: 1 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #33\n",
            "Path length: 2 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #34\n",
            "Path length: 36 reward: 300\n",
            "-----------------------------------\n",
            "Training iteration #35\n",
            "Path length: 1 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #36\n",
            "Path length: 6 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #37\n",
            "Path length: 41 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #38\n",
            "Path length: 68 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #39\n",
            "Path length: 4 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #40\n",
            "Path length: 2 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #41\n",
            "Path length: 13 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #42\n",
            "Path length: 12 reward: 300\n",
            "-----------------------------------\n",
            "Training iteration #43\n",
            "Path length: 39 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #44\n",
            "Path length: 26 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #45\n",
            "Path length: 1 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #46\n",
            "Path length: 17 reward: 300\n",
            "-----------------------------------\n",
            "Training iteration #47\n",
            "Path length: 2 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #48\n",
            "Path length: 1 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #49\n",
            "Path length: 7 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #50\n",
            "Path length: 2 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #51\n",
            "Path length: 6 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #52\n",
            "Path length: 3 reward: 300\n",
            "-----------------------------------\n",
            "Training iteration #53\n",
            "Path length: 28 reward: 400\n",
            "-----------------------------------\n",
            "Training iteration #54\n",
            "Path length: 1 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #55\n",
            "Path length: 33 reward: 300\n",
            "-----------------------------------\n",
            "Training iteration #56\n",
            "Path length: 4 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #57\n",
            "Path length: 1 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #58\n",
            "Path length: 478 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #59\n",
            "Path length: 8 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #60\n",
            "Path length: 2 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #61\n",
            "Path length: 60 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #62\n",
            "Path length: 39 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #63\n",
            "Path length: 13 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #64\n",
            "Path length: 8 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #65\n",
            "Path length: 13 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #66\n",
            "Path length: 29 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #67\n",
            "Path length: 288 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #68\n",
            "Path length: 61 reward: 300\n",
            "-----------------------------------\n",
            "Training iteration #69\n",
            "Path length: 6 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #70\n",
            "Path length: 4 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #71\n",
            "Path length: 1 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #72\n",
            "Path length: 29 reward: 300\n",
            "-----------------------------------\n",
            "Training iteration #73\n",
            "Path length: 1 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #74\n",
            "Path length: 1 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #75\n",
            "Path length: 27 reward: 500\n",
            "-----------------------------------\n",
            "Training iteration #76\n",
            "Path length: 174 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #77\n",
            "Path length: 11 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #78\n",
            "Path length: 20 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #79\n",
            "Path length: 43 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #80\n",
            "Path length: 47 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #81\n",
            "Path length: 71 reward: 400\n",
            "-----------------------------------\n",
            "Training iteration #82\n",
            "Path length: 3 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #83\n",
            "Path length: 35 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #84\n",
            "Path length: 17 reward: 300\n",
            "-----------------------------------\n",
            "Training iteration #85\n",
            "Path length: 51 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #86\n",
            "Path length: 6 reward: 300\n",
            "-----------------------------------\n",
            "Training iteration #87\n",
            "Path length: 14 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #88\n",
            "Path length: 6 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #89\n",
            "Path length: 15 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #90\n",
            "Path length: 185 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #91\n",
            "Path length: 1 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #92\n",
            "Path length: 12 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #93\n",
            "Path length: 2 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #94\n",
            "Path length: 104 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #95\n",
            "Path length: 8 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #96\n",
            "Path length: 308 reward: 100\n",
            "-----------------------------------\n",
            "Training iteration #97\n",
            "Path length: 6 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #98\n",
            "Path length: 102 reward: 200\n",
            "-----------------------------------\n",
            "Training iteration #99\n",
            "Path length: 13 reward: 200\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "for i in range(1, 100):\n",
        "    generate_random_map(f'input/input{1}.txt', min_n=2, max_n=6)\n",
        "    print('-----------------------------------')\n",
        "    print(f'Training iteration #{i}')\n",
        "    train(policy=policy, optimizer=optimizer, gamma=gamma, input_file=f'input/input{1}.txt', DEBUG=True, DEBUG2=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k14PJAyLQCh1"
      },
      "outputs": [],
      "source": [
        "torch.save(policy.state_dict(), 'policy.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1lP7MZj_Oh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75ebcac2-e597-48b6-a64d-5714294386e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path length: 46\n"
          ]
        }
      ],
      "source": [
        "solution, Total_reward = test(policy=policy, gamma=gamma, input_file=f'input1.txt', DEBUG=True, DEBUG2=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBMh4cjE_Oh4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f886941-f69a-4d73-a617-bd4444d8c46d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path length: 21\n"
          ]
        }
      ],
      "source": [
        "solution, Total_reward = test(policy=policy, gamma=gamma, input_file=f'input2.txt', DEBUG=True, DEBUG2=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSH9xbUo_Oh4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abffc5b8-6ed8-40bf-b833-2a7d0775f8fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path length: 88\n"
          ]
        }
      ],
      "source": [
        "solution, Total_reward = test(policy=policy, gamma=gamma, input_file=f'input3.txt', DEBUG=True, DEBUG2=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "48b5da803f30547ed7b9d121571e1e13db8f94548a0e004f74a7fbae0d9d83ab"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}